{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8b7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb26b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 'Acrobot-v1' environment\n",
    "environ = gym.make('Acrobot-v1')\n",
    "environ.seed(1)\n",
    "observe = environ.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906ec1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 12:17:12.294025: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "in_shape = environ.observation_space.shape[0]\n",
    "n_act = environ.action_space.n\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(in_shape,))\n",
    "hid_act = layers.Dense(128, activation=\"relu\")(inputs) \n",
    "hid_crit = layers.Dense(128, activation=\"relu\")(inputs) \n",
    "actor = layers.Dense(n_act, activation=\"softmax\")(hid_act)\n",
    "critic = layers.Dense(1)(hid_crit) \n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[actor, critic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c1b265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 12:17:12.917 Python[22343:1652883] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to (null)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "running reward: -498.60 at episode 10\n",
      "running reward: -406.69 at episode 20\n",
      "running reward: -273.48 at episode 30\n",
      "running reward: -203.99 at episode 40\n",
      "running reward: -157.23 at episode 50\n",
      "running reward: -128.87 at episode 60\n",
      "running reward: -117.04 at episode 70\n",
      "Solved at episode 72!\n"
     ]
    }
   ],
   "source": [
    "discount_rate = 0.99 # .99 Discount_Factor/Gamma\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Try ADAM optimizer\n",
    "loss_fn = keras.losses.mean_absolute_error # loss function\n",
    "episode_count = 0\n",
    "steps_per_epi = 500 #max \n",
    "running_reward = -steps_per_epi +1\n",
    "while True:  \n",
    "    state = environ.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        for timestep in range(1, steps_per_epi-1): \n",
    "            environ.render()\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            action_probs, critic_value = model(state)\n",
    "            \n",
    "            \n",
    "            action = np.random.choice(n_act, p=np.squeeze(action_probs)) # The weights of the Neural Network\n",
    "                                                                         # will modify the distribution determining\n",
    "                                                                         # our policy\n",
    "\n",
    "            next_state, reward, done, _ = environ.step(action)  # We take an action determined by the actors policy \n",
    "                                                                # structure and end up recieving a reward\n",
    "            \n",
    "            state_hold = next_state\n",
    "            next_state = tf.convert_to_tensor(next_state)       \n",
    "            next_state = tf.expand_dims(next_state, 0)\n",
    "            \n",
    "            \n",
    "            next_action, next_critic_value = model(next_state)  # Sample an action from our estimation of the policy\n",
    "                                                                # and value approximators\n",
    "            \n",
    "            TD_target = (reward + (1 - done) * discount_rate * next_critic_value) #TD target\n",
    "            \n",
    "            critic_loss = loss_fn(tf.expand_dims(critic_value, 0), tf.expand_dims(TD_target, 0)) # Critic loss\n",
    "            \n",
    "            diff = next_critic_value[0,0] - critic_value[0,0]\n",
    "            actor_score = -tf.math.log(action_probs[0, action])*diff \n",
    "                                                                # The actual actor score is the gradient of 'actor_score'. \n",
    "                                                                # The policy gradient is the expect value of \n",
    "                                                                # this gradient multiplied by diff.\n",
    "            loss_value = actor_score + critic_loss\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            state = state_hold\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward \n",
    "\n",
    "\n",
    " \n",
    "    # Print human readable development\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "        \n",
    "\n",
    "    if running_reward > -115:  # Defined criterion for approximately optimal policy\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc988b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create this function to test the agent below\n",
    "def one_step(environ, state):\n",
    "    state = tf.convert_to_tensor(state)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_probs, critic_value = model(state)\n",
    "    action = np.random.choice(n_act, p=np.squeeze(action_probs))\n",
    "    state, reward, done, _ = environ.step(action) \n",
    "    environ.render()\n",
    "    return state, reward, done, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defe9962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "# Let's test the agent. Take 1. See READ_ME file in the Deep Actor Critic repository\n",
    "# Test 1\n",
    "observe = environ.reset()\n",
    "steps_per_test = 500\n",
    "for step in range(1,steps_per_test):\n",
    "    observe, reward, done, info = one_step(environ, observe)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826ee94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "# Test 2\n",
    "observe = environ.reset()\n",
    "steps_per_test = 500\n",
    "for step in range(1,steps_per_test):\n",
    "    observe, reward, done, info = one_step(environ, observe)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7c59e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "# Test 3\n",
    "observe = environ.reset()\n",
    "steps_per_test = 500\n",
    "for step in range(1,steps_per_test):\n",
    "    observe, reward, done, info = one_step(environ, observe)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760aa953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "# Test 4\n",
    "observe = environ.reset()\n",
    "steps_per_test = 500\n",
    "for step in range(1,steps_per_test):\n",
    "    observe, reward, done, info = one_step(environ, observe)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a23a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "# Test 4\n",
    "observe = environ.reset()\n",
    "steps_per_test = 500\n",
    "for step in range(1,steps_per_test):\n",
    "    observe, reward, done, info = one_step(environ, observe)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
